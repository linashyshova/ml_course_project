{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import multilabel_confusion_matrix as confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_set = data.sample(frac=0.9)\n",
    "train_set_features = train_set.loc[:, train_set.columns != 'label']\n",
    "\n",
    "validation_set = data.drop(train_set.index)\n",
    "validation_set_features = validation_set.loc[:, validation_set.columns != 'label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lshyshova/.conda/envs/ml_course_project/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(max_iter=1000000000, multi_class='multinomial', solver='newton-cg').fit(train_set_features, train_set['label'])\n",
    "\n",
    "# with open('model_serialized', 'wb') as ouf:\n",
    "#     pickle.dump(logreg, ouf)\n",
    "\n",
    "logreg = 0\n",
    "with open('../logreg/model_serialized.file', 'rb') as inpf:\n",
    "    logreg = pickle.load(inpf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9521428571428572"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(validation_set_features)\n",
    "result = (validation_set['label'] == prediction).to_numpy()\n",
    "count_true = np.count_nonzero(result == True)\n",
    "count_total = len(result)\n",
    "accuracy = count_true / count_total\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction - count       prediction  label\n",
      "16             5      0\n",
      "1137           5      0\n",
      "1244           5      0\n",
      "Count - prediction      prediction  label\n",
      "540           0      5\n"
     ]
    }
   ],
   "source": [
    "combined = pd.DataFrame(prediction, columns=['prediction'])\n",
    "combined = combined.assign(label=validation_set['label'].values)\n",
    "count1 = combined[(combined.prediction == 5) & (combined.label == 0)]\n",
    "count2 = combined[(combined.prediction == 0) & (combined.label == 5)]\n",
    "print(\"Prediction - count\", count1)\n",
    "print(\"Count - prediction\", count2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9521465149457015\n",
      "Recall    : 0.9521428571428572\n",
      "F-score   : 0.9521130646329956\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(validation_set['label'], prediction)\n",
    "precision, recall, fscore, support = score(validation_set['label'], prediction, average='weighted')\n",
    "print('Precision : {}'.format(precision))\n",
    "print('Recall    : {}'.format(recall))\n",
    "print('F-score   : {}'.format(fscore))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[414,   0,   0,   0,   0,   3,   1,   0,   3,   1],\n       [  0, 522,   1,   1,   0,   0,   0,   1,   1,   0],\n       [  1,   4, 417,   6,   5,   0,   1,   6,   6,   1],\n       [  1,   0,   7, 376,   0,   7,   0,   3,   5,   5],\n       [  0,   1,   2,   0, 404,   0,   1,   2,   3,   7],\n       [  1,   1,   1,   6,   2, 331,   3,   0,   7,   4],\n       [  1,   0,   4,   1,   1,   4, 365,   0,   0,   0],\n       [  1,   1,   3,   3,   4,   0,   1, 414,   1,  11],\n       [  1,   5,   2,  11,   0,   6,   1,   0, 363,   3],\n       [  2,   0,   1,   2,   7,   0,   0,  12,   1, 393]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = confusion_matrix(validation_set['label'], prediction)\n",
    "matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
